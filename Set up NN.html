<h1>How to Build Neural Network</h1>
<h2>Works Like our Brain</h2>
<p><img src="https://d14rmgtrwzf5a.cloudfront.net/sites/default/files/images/NEURON2.gif" alt="Image of Single Neuron In Brain"></p>
<ul>
<li>Each neuron receive input from other neurons or receptors</li>
<li>The effect of each input on the neuron is constrained by synaptic weight</li>
<li>The synaptic weight adapt to allow whole neural network learn how to perform necessary task</li>
</ul>
<h2>Model of Artificial Single Neuron</h2>
<p><img src="http://www.lce.hut.fi/research/eas/compneuro/projects/cerebellum/neuron_model.gif" alt="Image of model of Single Neuron"></p>
<ul>
<li>Step 1: Compute the output given the initial weight</li>
<li>Step 2: Compare the output with actual label</li>
<li>Step 3: If the predicted label is wrong, update weight vector</li>
<li>Step 4: Repeat step 1-3 until the weight vector converges</li>
</ul>
<h3>Activation Function </h3>
<h4>Sigmoid Neuron</h4>
<ul>
<li>Formula: y = 1 / (1 + exp(-x))</li>
<li>Function in TensorFlow:tf.sigmoid(x,name=None)</li>
</ul>
<h4>Tanh Neuron</h4>
<ul>
<li>Formula: Compute hyperbolic tangent of x</li>
<li>Function in TensorFlow:tf.tanh(x, name=None)</li>
</ul>
<p>Information about more activation function could be found at <a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/activation_functions_">here</a>.</p>
<h2>Feedforward Neural Network</h2>
<p><img src="http://web.utk.edu/~wfeng1/spark/_images/fnn.png" alt="Image of Feedforward Neural Network"></p>
<ul>
<li>Could be multi-layered, and perceptron are arranged in layers</li>
<li>The neuron in one layer only connect the neuron in subsequent layer</li>
<li>No connection between the neuron in same layer</li>
<li>The output simply depends on the current input</li>
<li>Easy to implement</li>
</ul>
<h3>Code Example (Simple XOR Problem):</h3>
<pre><code class="language-javascript">X = [[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]]
Y = [[<span class="hljs-number">0</span>], [<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>], [<span class="hljs-number">0</span>]]
input_layer = input_data(shape=[None, <span class="hljs-number">2</span>])
hidden_layer = fully_connected(input_layer , <span class="hljs-number">2</span>, activation=<span class="hljs-string">'tanh'</span>) 
output_layer = fully_connected(hidden_layer, <span class="hljs-number">1</span>, activation=<span class="hljs-string">'tanh'</span>) 
regression = regression(output_layer , optimizer=<span class="hljs-string">'sgd'</span>, loss=<span class="hljs-string">'binary_crossentropy'</span>, learning_rate=<span class="hljs-number">5</span>)
model = DNN(regression)
model.fit(X, Y, n_epoch=<span class="hljs-number">5000</span>, show_metric=True)
[i[<span class="hljs-number">0</span>] &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> model.predict(X)]
</code></pre>
<p>More details of this example code could be found at <a href="https://towardsdatascience.com/tflearn-soving-xor-with-a-2x2x1-feed-forward-neural-network-6c07d88689ed">here</a>.</p>
<h2>Recurrent Nueral Network</h2>
<p><img src="https://i.stack.imgur.com/KmrmP.png" alt="Image of Recurrent Neural Network"></p>
<ul>
<li>Have directed cycle in the connection</li>
<li>The neuron in one layer may connect to the neuron in same layer</li>
<li>The output depends on the current input and previous output</li>
<li>Good for modeling sequential data (eg. time series/sequential task)</li>
</ul>
<h3>Code Example (Language Modelling Problem):</h3>
<p>This RNN example can be used to predict the next word given the history of previous word.</p>
<pre><code class="language-javascript"> t=<span class="hljs-number">0</span>  t=<span class="hljs-number">1</span>    t=<span class="hljs-number">2</span>  t=<span class="hljs-number">3</span>     t=<span class="hljs-number">4</span>
[The, brown, fox, is,     quick]
[The, red,   fox, jumped, high]

words_in_dataset[<span class="hljs-number">0</span>] = [The, The]
words_in_dataset[<span class="hljs-number">1</span>] = [fox, fox]
words_in_dataset[<span class="hljs-number">2</span>] = [is, jumped]
words_in_dataset[<span class="hljs-number">3</span>] = [quick, high]
num_batches = <span class="hljs-number">4</span>, batch_size = <span class="hljs-number">2</span>, time_steps = <span class="hljs-number">5</span>
final_state = state
words_in_dataset = tf.placeholder(tf.float32, [num_batches, batch_size, num_features])
lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
# Initial state of the LSTM memory.
hidden_state = tf.zeros([batch_size, lstm.state_size])
current_state = tf.zeros([batch_size, lstm.state_size])
state = hidden_state, current_state
probabilities = []
loss = <span class="hljs-number">0.0</span>
<span class="hljs-keyword">for</span> current_batch_of_words <span class="hljs-keyword">in</span> words_in_dataset:
    # The value of state is updated after processing each batch of words.
    output, state = lstm(current_batch_of_words, state)

    # The LSTM output can be used to make next word predictions
    logits = tf.matmul(output, softmax_w) + softmax_b
    probabilities.append(tf.nn.softmax(logits))
    loss += loss_function(probabilities, target_words)
</code></pre>
<p>More details of this example code could be found at <a href="https://www.tensorflow.org/tutorials/recurrent">here</a>.</p>
